[{"uri":"/posts/","title":"A Blog","tags":[],"description":"","content":"  Transactional MFA Sequence     Protecting Change With MFA     Retro for Value     You Are Valued     Retro Is Broken     Clean Cloud     Notes on a stack     Begin With Disaster     TEST is a four letter word.     Reliable, Evolving, Agile, Lean     100% Coverage is only the beginning     "},{"uri":"/posts/transactional-mfa-sequence/","title":"Transactional MFA Sequence","tags":[],"description":"","content":"CHANGE action with 2nd Authentication Factor This sequence is explored in detail in the Protecting Change With MFA post.\n\u0026lsquo;Happy path\u0026rsquo; sequence %%{ init: {\u0026quot;theme\u0026quot;:\u0026quot;dark\u0026quot;}}%%\rsequenceDiagram\rparticipant USER as User1\rparticipant UI as User Interface\rparticipant API as API\rparticipant IDP as Secure Token Service\rparticipant BACKEND as Backend System\rnote over USER,API: Authenticated user on 'Home' page.\ractivate UI\rUI -\u0026gt;\u0026gt; USER: DISPLAY \u0026quot;Home\u0026quot;\rdeactivate UI\rnote right of UI: A menu contains\u0026lt;br/\u0026gt;the \u0026quot;Changes\u0026quot; item.\rUSER -\u0026gt;\u0026gt; UI: CLICK \u0026quot;Changes\u0026quot;\ractivate UI\rUI -\u0026gt;\u0026gt; USER: DISPLAY \u0026quot;Changes Page\u0026quot;\rdeactivate UI\rUSER -\u0026gt;\u0026gt; UI: CLICK \u0026quot;New Change\u0026quot;\ractivate UI\rUI -\u0026gt;\u0026gt; API: OPTIONS /change-thing\rdeactivate UI\ractivate API\rnote right of API: CHANGE is a protected \u0026lt;br/\u0026gt; action. Requires \u0026lt;br/\u0026gt; 2nd factor.\rAPI -\u0026gt;\u0026gt; UI: HTTP 401\ractivate UI\rdeactivate API\ractivate IDP\rUI --\u0026gt;\u0026gt; IDP: Initiate MFA\rdeactivate UI\ractivate IDP\rIDP --\u0026gt;\u0026gt; USER: Deliver 2nd Factor\rdeactivate IDP\ractivate UI\rUI -\u0026gt;\u0026gt; USER: SHOW MFA Form\rdeactivate UI\rUSER -\u0026gt;\u0026gt; UI: ENTER 2nd Factor\ractivate UI\rUI -\u0026gt;\u0026gt; IDP: GET /token?id=User1\rdeactivate UI\ractivate IDP\rIDP -\u0026gt;\u0026gt; UI: HTTP 200 {token: 'X'}\rdeactivate IDP\ractivate UI\rUI -\u0026gt;\u0026gt; API: POST /change-thing {token: 'X', change: Object}\rdeactivate UI\ractivate API\rAPI -\u0026gt;\u0026gt; BACKEND: POST /change-thing\rdeactivate API\ractivate BACKEND\rBACKEND -\u0026gt;\u0026gt; API: HTTP 200 { response: Object}\ractivate API\rdeactivate BACKEND\rAPI -\u0026gt;\u0026gt; UI: HTTP 200 {state: Object}\rdeactivate API\ractivate UI\rUI -\u0026gt;\u0026gt; USER: SHOW \u0026quot;Results Page\u0026quot;\rdeactivate UI\r"},{"uri":"/","title":"wbits","tags":[],"description":"","content":"Welcome to the blog of wbits.\n A Blog    Transactional MFA Sequence   Protecting Change With MFA   Retro for Value   You Are Valued   Retro Is Broken   Clean Cloud   Notes on a stack   Begin With Disaster   TEST is a four letter word.   Reliable, Evolving, Agile, Lean   100% Coverage is only the beginning    "},{"uri":"/posts/protecting-change-with-mfa/","title":"Protecting Change With MFA","tags":[],"description":"","content":"Using transactional MFA to protect mutative actions When deciding whether or not to use transactional MFA to verify the identity of a user the tradeoffs are around implementation security, useability and simplicity.\nTransactional MFA is the protection of specific actions by challenging for a second authentication factor. In general, these are transactional actions such as CHANGE some data. Like, to pay for something, alter a subscription, update a user\u0026rsquo;s address.\n There is a level of sophistication (complexity) to transactional MFA that may lead to some appearance of crossover to Authorisation. While authorisation is about allowing certain entities to access a given action based on their role, transactional MFA is about increasing the amount of identification required to perform the action.\nFor example, to rent a video you probably only need a telephone bill, whereas to buy a big TV on credit you need a drivers\u0026rsquo; licence and a credit card. Anyone can do these things, they just require more proof of identity.\n\u0026ldquo;Easy MFA\u0026rdquo; - Challenge at Login In the simplest implementation and user journey, MFA is a part of the login process. Once the user is MFA\u0026rsquo;d at login, their identity is assumed to be constant for the duration of the session.\n%%{init: {'theme': 'dark'}}%%\rsequenceDiagram\rparticipant USER as User\rparticipant UI as UI\rparticipant API as API\rparticipant MFA as Identity Provider\rUI -\u0026gt;\u0026gt; USER: SHOW \u0026quot;Sign Up / Sign In\u0026quot;\rnote over USER,UI: Enter Username / Password\rUSER -\u0026gt;\u0026gt; UI: CLICK \u0026quot;Sign In\u0026quot;\rUI --\u0026gt;\u0026gt; API: GET /user-mfa\rAPI --\u0026gt;\u0026gt; UI: {true|false}\ralt MFA ENABLED\rnote over USER,MFA: Some complicated stuff\relse MFA NOT ENABLED\rnote over USER,MFA: Nothing to do\rend\rUI -\u0026gt;\u0026gt; USER: SHOW Home Page Dashboard\rUSER --\u0026gt;\u0026gt; UI: Click \u0026lt;any button\u0026gt;\rnote over USER,API: Any action is allowed from now on\rA more detailed look at transactional MFA A sequence diagram showing the 2nd factor guard in action.\nThree Tradeoffs of MFA at Login    Tradeoff Status     Security Some risk: for example somebody logs in at a computer in say, a classroom or office, and walks away without locking their screen or logging out.   Useability Extra time spent waiting and extra buttons clicked, text copied-and-pasted (or worse, read from the phone and typed into the computer) and some users will find that disagreeable.   ✅ Simplicity The implementation is at its simplest when MFA\u0026rsquo;ing at login.    The Example System The application uses an external system to apply changes to say, a user\u0026rsquo;s profile data. Items such as \u0026ldquo;nickname\u0026rdquo;, \u0026ldquo;favourite colour\u0026rdquo; etc are stored in the external system (it doesn\u0026rsquo;t matter how or where).\nWith transactional MFA implemented, changes to data are protected against naughtiness using multi-factor authentication. The second factor could be an SMS text, an authenticator app (such as Google Authenticator) or a one-time passcode sent to the user\u0026rsquo;s email.\nREAD or COMPUTE are not \u0026lsquo;sensitive\u0026rsquo; When applying transactional MFA, non-mutative actions are in general not protected using the second factor. An action such as, view_account_balance is not transactional, it is a VIEW. A calculated action, such as estimated_gas_bill won\u0026rsquo;t need MFA either.\nThese actions are protected using the credentials factor alone. Username / password is considered to be sufficient protection for READ and COMPUTE operations, but a second factor is considered necessary to guard against potentially site-wide harm that may be caused by a bad actor exploiting a mutative process.\nIt depends entirely on the domain of course, a banking application may have much more strict requirements than a utility providing gas, electricity and internet or a fishing club.\nNonrepudiation and identity binding The second factor will not provide complete protection but it will provide added protection.\nA bad actor would have to have compromised both factors to gain access, which \u0026hellip; might be less likely than the account owner being the perpetrator of the naughty thing.\nMore identifying factors should increase the likelihood of a verified identity. But they might not.\n In this case, the system is presenting a final alert, \u0026ldquo;are you sure you want to do this?\u0026rdquo;, as a warning and as a means of identification.\nOne time MFA - still a threat The diagram above describes a posture where convenience is still a high priority. A user may wish to perform multiple sensitive actions, but will only be challenged for the 2nd factor the first time. Subsequent sensitive actions are pre-authenticated on the 2nd factor.\nThree Tradeoffs of One Time Transactional MFA Security is relaxed somewhat for READ operations. Of course it is still possible to require a 2nd factor at any point, the system in question has chosen to apply them only to mutative actions.\n   Tradeoff Status     Security The securedness of mutative operations is improved. At the time of the action, rather than at some time prior, the user is challenged to provide the 2nd factor. However, should a user authenticate with the 2nd factor, then walk away and leave their computer unlocked and the session open their account is just as open to attack as if they had provided the 2nd factor at login.   ✅ Useability Useability is improved site-wide. For a user who does not want to perform mutative actions (such as VIEW their account balance) a single authentication factor is enough. Therefore, the user does not have to wait or fiddle about with obtaining and providing the 2nd factor.   Simplicity Simplicity is foregone. In order to implement transactional MFA around only mutative actions, a lot of extra presentation logic is required. If there is no BFF the API itself must include challenge logic in every POST/PUT/PATCH/DELETE that is exposed. There are ways to cross-cut this, but they are still complex / complicated.    All Ye Muste MFA the things    Tradeoff Status     ✅ Security If security is the highest concern, it makes sense to have every sensitive action protected by a MFA challenge. Making the system stateless in that sense will be very inconvenient for the user, as they will need to provide the 2nd factor every time they attempt a sensitive action but it will give the most clarity around who is attempting to perform the action.   Useability Useability takes a back-seat to security in this configuration, and each and every time the user wants to perform a sensitive action they must deal with the hassle of providing the 2nd factor. There are some apps that make this easier - but security and convenience are mutually exclusive.   Simplicity It\u0026rsquo;s actually somewhat simpler to implement the stateless MFA for all sensitive actions as there is no requirement to check the state of 2nd factor - it\u0026rsquo;s always in a state of \u0026ldquo;not provided\u0026rdquo; for this implementation     The trade-offs are clear. It is a matter of posture and policy At the end of the day, to 2FA or not to 2FA is the question ultimately answered by the stakeholders - and the better equipped to make that decision they are, the more appropriate to the user base that decision will be.\nAs with everything, \u0026ldquo;Ye Muste Use the MFA\u0026rdquo; or \u0026ldquo;MFA Suxe\u0026rdquo; are extreme views, and more often than not the real-world answer is\u0026hellip;.\nThere are other implementation concerns too. For example, considering certain RBAC grants are only available to MFA entities. Once the 2nd factor is presented, a raft of new permissions are available, similarly to how we might sudo or elevate our role permissions temporarily on a cloud platform.\nReferences Thank you to the writers of these articles:\nMulti-Factor Authentication Sucks\nNever Build MFA Again: A Developer\u0026rsquo;s Guide to Transactional MFA\nBeginner\u0026rsquo;s Guide to SAML\n"},{"uri":"/posts/retro-for-value/","title":"Retro for Value","tags":[],"description":"","content":"Retrospectives and the Tightest Possible Feedback Loop Retrospectives that happen infrequently, say every 6 weeks are less effective primarily due to the amount of time spent \u0026ldquo;doing it wrong\u0026rdquo;.\nThese habits become so ingrained and taken for granted that by the time retrospective comes around, the team is highly reluctant to change the way they are working and try to push back on any (now major) changes to the way of working that will cause a momentary stoppage in the flow of work.\nIt is this reluctance to take the time to re-evaluate the flow, to \u0026ldquo;slow down to speed up\u0026rdquo; that causes the inevitable spiral towards ever increasing velocity losses, build up of escaped bugs, bottlenecks and prioritisation errors characteristic of under-performing teams.\nSignals of imbalance Teams which, on paper, are staffed with highly capable and well-respected individuals. It is no one-person\u0026rsquo;s fault - it is the fault of an approach to team strategy and the failure to use appropriate metrics to signal corrections in course that will allow flow to return to expected velocities.\nBut how to identify these signals? By the time management recognises the failures of a given team to deliver value to the customer in a prompt, efficient and profitable way, it\u0026rsquo;s easy to start apportioning blame. The problem lies with the vendor. The problem is with the iteration manager. The team is too immature. The product owner is stuffing too much work into the backlog. QA is failing to find bugs.\nWhat if all of these things are true, but they are in fact not the reason the team is running so slowly? What if there was a way to compress the time spent actually doing the work into manageable chunks of time, without interruption and with an appropriate division of work items that catered to the three pillars of value: Features, Risks, Debt.\nInstead of the traditional trade-offs where the product owner (the business representative for the team) is forced to schedule defect-fixing over risk-reduction or technical debt before features - what if the strictest of adherence was paid to the balanced allocation of work items for the three pillars?\nInstead of applying pressure to developers to ship code more quickly or to the QAs to verify newly released features, what if developers were encouraged to spend extra time to ensure their code was thoroughly, and provably tested? What if QA was encouraged not only to automate the \u0026ldquo;usual\u0026rdquo; tests around a given feature, but encouraged to spend extra time to explore each item of work and apply creative, imaginative and rigorous techniques to break the build.\nWhat if a broken build was a thing to be celebrated? What if the person who broke the build was given the time and space, support and attention to really know what it was that caused the broken build in the first place? And, if the person who broke the build was a QA - what if the development team developed a creative way to catch that in their unit tests, patternised it - learned from it?\nSurely this sounds like a very bogged down process?\nBut what if this was the only process? What if every work item had its own retrospective?\nThis is the tightest feedback loop I can think of right now. It is these learnings that, once a week, are presented to the team for wider discussion and learning, incorporation into the way of working.\nSo the question remains, with all these ceremonies, is there any time for the actual work?\nNo more meetings Given the following iteration plan:\n   Day Actor Activity     -1 PO Selects one work item from each pillar, to commence on day 1   0 ALL Retrospective, lightning talks, sprint goal, 3A organisation   0 IM Assigns work items to 3-amigos for kickoff   1 3A Work item kicks off, 3A in high-bandwidth collaboration   1+ 3A Work item ships, retro   2+ 3A Applies learnings, writes enablement code, pays debt   2++ PO Selects one work item from the next pillar for each idle team    With a standup at the earliest convenient time for the team each morning as the only collective touchpoint and all other \u0026ldquo;meetings\u0026rdquo; are actually 3 Amigos tactical squads who are pairing on code, clarifying requirements, feeding back to PO, communicating with dependencies (until all external dependencies are brought into the 3A organisation - one-big-team) and spending equal time working on the three-pillars\u0026rsquo; aspects to any given feature.\nApplying all three pillars to every feature seems like wasteful recursion, but in fact it is this triple checking that ensures each feature is appropriately weighted with each of the value pillars before it ships.\nWhat this does is reduce the accumulation of Debt work items, and is a limiting factor on the Risk work items. The Big-Ticket work items are effectively sliced and grafted into every new feature, thus ensuring each of the three pillars is address from a macro and a micro perspective.\nTo make all of this work, a profound and breaking change to the way a team works is required. This is not capital-A Agile, this is not Devops, nor is it specifically Lean Thinking. It is a focused and tightly woven sequencing of play that takes each work item as a unit of flow, and applies the SERVICE_TEMPLATE pattern to it.\nThis has been inspired by the Flow Framework, and the three pillars of value are either stolen, misused or re-shaped from there.\nMaking it work The only way to get a team of any size adjusted to this kind of completely foreign construct is to spend some time in simulation.\nSimulation is the playing of games. Games bring the element of sport, friendly competition and helpfulness that accompanies low-stakes activities.\nSimulating the iteration plan outlined above is simple when applied to any number of traditional flow games.\nFolding paper aeroplanes, the beer game, even the game from The Goal. These simple games with (the fewer the better) simple rules are easy to understand but difficult to master. It is with the application of the plan outlined above that the team will start to learn how to operate in this new way, and grow used to it without the pressure of production, the constant increase in the actual duration of a given item of work from its estimation.\nSingle Piece Flow It does not make sense to use estimation to plan a set of work items for a given \u0026ldquo;sprint\u0026rdquo;. This turns the estimation into a deadline, no matter how it is framed. Once there is a deadline, pressure forms around the item of work. That pressure causes cracks to form in the veneer of quality and debt will accrue as less time is spent thinking about how the item of work would be designed for the future. Security is overlooked in favour of convenience, and bugs escape to production.\nInstead, feeding items of work one at a time, allows the team to feel relaxed and valued. They are not being questioned about \u0026ldquo;how long\u0026rdquo; and \u0026ldquo;why is it not done yet\u0026rdquo; because no single item of work is resting on a single person\u0026rsquo;s shoulders - it is the 3 Amigos construct that allows the responsibility to be shared and therefore each player is able to step in when another is starting to lose momentum (either tiredness, knowledge gap, screaming kids) and fire up fresh engines. This is made possible by all 3 Amigos being allowed to focus on a single item of work.\n"},{"uri":"/posts/you-are-valued/","title":"You Are Valued","tags":[],"description":"","content":"Sincerity is meaningless In today\u0026rsquo;s workplace, a lot of emphasis is placed on the wellbeing of workers. In fact, workers are not supposed to be called workers. Resources are people, people are participants.\nWhen this kind of cultural shift is applied across the social spectrum of an organisation, the effects are widespread - not all of them are the expected outcomes of inspiration, motivation, equality, safety.\nThere are negative effects too, dangerous effects to productivity.\nWhen the focus shifts to the people of an organisation, focus is shifting away from the customer. Losing sight of the customer will cause a fragmentation of the shared objective: delivering customer value.\nFor a participant to feel valued, it is simply not enough to tell them so. In movies, there is a saying. \u0026ldquo;Don\u0026rsquo;t tell the audience, show the audience\u0026rdquo;.\nShow the audience By telling participants they are valued, there is a risk that those very people who you are trying to reach will in fact disengage. When the message is overtly stated there is a hollowness to it - and sensitive people will detect (or infer) that it is an attempt to reassure, rather than any kind of actual cultural movement.\nKindness, supportiveness - \u0026ldquo;all that hippy stuff\u0026rdquo; (to quote Elizabeth Hendrickson) do not, alone, convey the whole message. It is not enough to simply say to the employees of an organisation that they are valued. The question arises, \u0026ldquo;What part of me is valued?\u0026rdquo; - to work in an organisation that will reimburse you for a desk and chair is nice. But when your opinions are ignored, you don\u0026rsquo;t feel safe to deliver bad news, there\u0026rsquo;s no real collaborative connection with the other members of your team \u0026hellip; when your PO is actually the \u0026ldquo;boss\u0026rdquo; and every work item you deliver is late, buggy, or both it\u0026rsquo;s pretty tough to feel valued.\nThe diminishment of self-worth Combating this descent into self-worth depreciation is actually something that can be easily achieved. It simply means following through on the message. Organising ways of working that treat team members as the highly trained, highly skilled, creative and hard-working people that they are.\nEven the most junior members of a team ought to have a voice. Even when those team members raise issues that are \u0026ldquo;wrong\u0026rdquo; or ask questions that are years behind other members of the team is not cause for talking down to them, patronising them.\nWhen someone is late with an item of work they thought would ship earlier, it is not something that should be used to make them feel uncomfortable. It is actually a problem with the ways of working. People are not always wired up to take on work all alone, indeed a lot of people work better when in collaboration with others. High-bandwidth communication and keyboard-switching such as is seen with Pair Programming is an effective way to raise quality standards, build stronger knowledge across the whole team, and allow people to relax and get to know one another at work.\nPairs that don\u0026rsquo;t work Pair programming is weird. It either works, or it doesn\u0026rsquo;t work.\nIn all the cases I have seen where it doesn\u0026rsquo;t work, it is because there is nobody around who wants to make it work. It is because the pair not only feel unempowered in this situation, they actually have no knowledge of the concepts at play, have not had any exposure beyond the (all too) common \u0026ldquo;you know, pair up on this and work it out\u0026rdquo;.\nThe pair that doesn\u0026rsquo;t work is the one where one programmer ends up doing all the typing and the other feels increasingly disengaged, eventually pulling out their own laptop and doing something else.\nSo to set up a pair for success, they will need some support and time to prepare for their pairing. Some time to read about it, a few failed sessions, even some coaching. It will be worth it.\nPairs that work Pairs that work are those where the programmers (or at least one of them) has seen it before, read about it before, participated before.\nWhen a pairing succeeds it is because the participants are comfortable to match pace. If there is a super-fast programmer paired with a not-so-super-fast programmer, they slow to the speed of the not-so-fast programmer naturally, so as to maintain the integrity of the pairing.\nSometimes the super-fast programmer will whip up a test for the slower programmer, and pass the keyboard. The slower programmer might apologise for being slower but the pair that works is one where the apology is accepted but not necessary. Both programmers are not just producing work here. They are investing in the growth and knowledge of the whole team.\nIt\u0026rsquo;s a long-game strategy.\nRetros that listen When retrospectives are held, it is vital that there are clear definitions of what a retro-item ought to be. Focus on \u0026ldquo;team work\u0026rdquo; and congratulatory messages are not actually what makes people feel valued. This may be difficult to digest initially, even counter-intuitive especially given the \u0026ldquo;people not resources\u0026rdquo; culture in vogue - but for people to feel valued the one thing that will carry that message all the way home, every time is listening.\nAs long as a retro item is focused on delivery of customer value, it is appropriate to attempt to implement the person\u0026rsquo;s suggestion. Give the suggestion some time to succeed or fail, so that all may learn from it.\nThere will be times when more senior team members will be tempted to scoff at the suggestions of more junior members, but with the benefit of tighter feedback loops these suggestions can be implemented in earnest and their effects observed, safely.\nSafety and Failure There is nothing wrong with suggesting something that will fail. There is always the possibility that a suggestion that appears on the surface to be naive and uninformed will actually turn out to succeed even if those who are affected by the suggestion find a deeper inspiration from it and come up with something truly beneficial in response.\nThese types of experiments, conducted by the team, will only serve to grow their collective experience. Of course, this works best with long-lived teams but in the case of teams with some turnover the knowledge-waste will at some point become the signal to management that more attention needs to be paid to the length of a given team\u0026rsquo;s grouping.\nIf it is truly safe to fail then each team member will feel more than just valued they will feel trusted. A person who feels trusted is less likely to consider themselves unempowered, less likely to question their own self-worth, and more likely to love their work and their workplace.\nTrust When management trusts the teams to tend towards success, failures become milestones on that team\u0026rsquo;s path to success. As more and more successful improvements are made, the team is attracted to an equilibrium whereby the flow of work is increasingly unhindered, until value is being released at a near-optimal rate.\nWhen the team is given the clear objective of optimising for delivery of customer value and each team member is truly valued and trusted in word and in deed, the delight they feel as they produce customer value will be transmitted, in kind, to the customer.\n"},{"uri":"/posts/retro-is-broken/","title":"Retro Is Broken","tags":[],"description":"","content":"Delivery teams are focusing on the Wrong Things I have participated in more Retrospectives than I can count. Years and years of working with dev teams has given me an insight into some antipatterns which are causing the failure of not just continuous improvement, but are causing teams to actually diminish their effectiveness and productivity.\nAntipattern #1 : It\u0026rsquo;s all about US So many times I have seen super-awesome change-agents, transformation coaches, Agile leaders, practice leads (and more) who are playing the \u0026ldquo;popularity contest\u0026rdquo;.\nThis is disguised as \u0026ldquo;a people-centric approach\u0026rdquo; or \u0026ldquo;it\u0026rsquo;s the teams who are the most important\u0026rdquo; etc. It is a beautiful sentiment, and makes the participants feel special, important - valued. This is great, it\u0026rsquo;s good for morale, and as they say, \u0026ldquo;happy workers work harder\u0026rdquo;.\nYet - focusing on the people has a significant negative impact on the outcome of any transformation effort. This negative impact is brought about by re-orienting the focus of the delivery teams at themselves rather than the customer.\nBy focusing on themselves, retrospectives tend to be filled with these kinds of lean-coffee-tickets:\n Team lunch was great :) Sorry to see John leaving this week :( Only 5 broken builds this sprint! :) Well done team! We smashed 10 bugs! :) Why are we using Jenkins not CodeBuild ?  Etc\u0026hellip;\nNot one of these tickets will result in an action that enables greater throughput or reduces wait times, more on this later.\nAntipattern #2 : The Three Wrong Columns Almost all the retrospectives I have witnessed have some variant on this:\n1. What went well\r2. What didn't go well\r3. What could be better\r Some creative facilitators use a sailing ship analogy:\n1. What put wind in the sails\r2. What dragged us like an anchor\r3. What could scrape barnacles off the hull\r It\u0026rsquo;s all the same. And none of it has anything to do with continuous improvement, transformation, or VALUE. More on this later.\nAntipattern #3 : Five to Ten minutes to write up your tickets Lean coffee format. How many times have you heard this question:\n Are you familiar with the Lean Coffee format?\n Followed by a sequence of furrowed brows, academic-sounding recitals of such-and-such a blog post, possibly dropping the name of x, y, or z consultancy and capital-A Agile methodologies of course. Lean Coffee is, after all, what ALL the Agilists are using.\nThe Lean Coffee approach is fundamentally flawed if applied to retrospective. Putting participants on the spot basically forces them to come up with anything they can think of. This is what leads to the \u0026ldquo;team lunch :)\u0026rdquo; tickets everywhere.\nThe team is working really hard every day, when it comes to retrospective they are relaxing for the first time in probably two weeks. It is unrealistic to expect that all the members of the team will remember things that happened 8+ days ago that had an impact on their productivity, especially if the areas of focus are vaguely, or not at all (like most places I have worked) defined.\nCountermeasures #1 - Understanding what Continuous Improvement is actually FOR Defining a target for improvement sounds obvious doesn\u0026rsquo;t it? If the target was, \u0026ldquo;make sure the team is really friendly with one another\u0026rdquo; then yes, putting the participants on the spot with 5 minutes of Lean Coffee ticket-writing at the start of the retrospective is fine.\nWhen the REASON for a transformation is actually (sensible) to realise value sooner, unblock the value stream, minimise waste and deliver value TO THE CUSTOMER more efficiently then suddenly \u0026ldquo;team lunch :)\u0026rdquo; type tickets have very little reason to appear.\nBy working with the team(s) to develop a deep understanding of their professional and commercial objectives within the wider transformative (or continued) improvement effort will have an impact on the team\u0026rsquo;s self-image as highly-skilled, important and VALUED participants.\nDeclaring these objectives and framing them as PROFESSIONAL conduct, part of any team member\u0026rsquo;s role and responsibilities will elevate that team member\u0026rsquo;s idea of their place in the business. Not just \u0026ldquo;we all love you, you are \u0026lsquo;valued\u0026rsquo;\u0026rdquo; (which is warm and fuzzy, but it\u0026rsquo;s vague, and \u0026hellip; well, fluff) - but providing tangible, definitive explanations of these words at a professional level.\n#2 - Collecting Improvement Items Continuously This is another obvious-seeming activity, but I have witnessed delivery team members actually push back on this suggestion claiming that it is \u0026ldquo;not worth it\u0026rdquo;.\nBy placing a post-it stack at every team member\u0026rsquo;s workstation (I have seen the digital equivalent of this tried a lot, and fails every time) and reminding the team members to write down something that just happened that ought to be improved the items that are presented at retro are real-life events, VALUABLE to the team.\nThis requires some coaching at the outset, where the team is still learning the meaning of \u0026ldquo;the 7 types of waste\u0026rdquo; (for example) and needs some encouragement to take ownership of their team\u0026rsquo;s destiny, so to speak.\nWhen something happens that causes problems, such as:\n* too many forms to fill in\r* too many clicks, pages and noise to release something\r* a person's \u0026quot;doing\u0026quot; column filling up too fast\r* too many bugs causing work items to go back to dev\r* the system architecture and organisation layout causes one team to wait for another\r* too many meetings!\r* etc\r These events can be written onto post-its at the time so they are not forgotten. Writing these up as they happen also improves the quality of the information around these moments, such as knowing \u0026ldquo;how many minutes/hours I had to wait\u0026rdquo; for example.\nWhere it is simply too much hassle to convince people to keep notes on their desk, something I have seen working is for the coach / scrum master / etc to nominate themselves as the official recipient of retro items.\n\u0026ldquo;Send me an email when you encounter a problem\u0026rdquo; This enables team members to write a detailed account of their issue, and know that it will be actioned and possibly even investigated for retro. This will improve even further their sense of being valued, listened to, taken seriously.\n#3 - Positive Reinforcement Is Still Important Earlier I noted that the format of retrospective is broken, however for a process of continuous improvement to succeed improvement actions should be monitored.\nThis is what the \u0026ldquo;what went well\u0026rdquo; column might have originally been used for?\nTo repurpose this column for the checking of improvement actions will enable the team to gain insight into the effectiveness of their improvement initiatives. If an action meant to improve productivity (see countermeasure #1) is not improving productivity, or has an unforeseen impact on another person or process then it is up to the team to re-visit this improvement and re-work it, try try again.\nYes People Do Matter Of course people matter, their wellbeing ought to be at the forefront of everyone\u0026rsquo;s mind! Each member of the team ought to be considering the health and wellbeing of all of their team mates, and everyone the interact with in the world.\nIn terms of Agile ceremonies, while coaching has placed a special emphasis on things like \u0026ldquo;respect\u0026rdquo;, this emphasis has become the focus.\nHealth, happiness and wellbeing ought to be woven into the fabric of the social side of \u0026ldquo;when we are at work together\u0026rdquo;.\nHappy workers work harder, sure. But even the happiest workers in the galaxy will still produce less when their delivery is hampered by irregular work items, overburdened process, unnecessary waiting, and those people who are frustrated by these things will in fact, become less happy as a result.\n"},{"uri":"/posts/clean-cloud/","title":"Clean Cloud","tags":[],"description":"","content":"A set of things that can be easily assimilated into an enterprise IT culture Enterprise IT is rife with litter.\nThere are EC2 instances running for months, called TEST001.\nThere are S3 buckets with no objects in them, that have no tags.\nThere are entire features without a single automated test.\nThere are items of work that do not have adequate acceptance criteria.\nThere are overprovisioned servers, long-running environments, bloated taxonomies \u0026hellip;\nOur clouds are not clean.\nThe Clean Cloud foundation relies on three core things:\n Test EVERYTHING to provide PROOF that it WORKS and is FIT FOR PURPOSE Automate EVERYTHING to provide healthy, right-sized infrastructure Continuously search for, identify, and eliminate WASTE.  On Point 1 - CONFIDENCE The collaboration between the business and IT has been reported to be unhealthy time and again - the business does not trust IT, \u0026ldquo;IT should be the ENABLER\u0026rdquo;, and this is truest when the business cannot act natively in the marketplace because their IT workloads are buggy, slow, unresponsive, expensive, unreliable - etc.\nBuild CONFIDENCE by providing rigorous, repeatable PROOF to the business that the software they WANT is the software they GOT On Point 2 - RISK Variation and unpredictability are key components driving up operational and even profit risk all over enterprise IT. When automated processes fail - they fail as SOFTWARE, thus they can be FIXED like software. The failure occurs, a test is written which proves the failure, then a fix is applied which passes the test. This method of RISK DRAINING and FAILURE REMOVAL is guaranteed to produce dependable infrastructure from front-end widgets to entire Line Of Business stacks that eventually cover the vast majority of failure modes through the application of EXPERIENCE-DRIVEN evolution.\nThe pace of that eventuality is a decision to be made by the business, and by IT. The more attention paid to reducing risk, the more risk is removed using this method.\nOn Point 3 - WASTE This is key. Waste occurs everywhere EVERYWHERE in the enterprise. From the coders participating in the latest new features to the leadership making decisions without adequate or appropriate measures and monitoring. There are \u0026ldquo;types of waste\u0026rdquo;, and every participant in the enterprise should be familiar with them, be vigilant for them and raise their identification of them at the earliest opportunity.\nRetrospectives are one way the capital-A Agile people address waste. Education on how best to identify and eliminate waste is one of the most powerful tools available to the enterprise to reduce spend, increase velocity, and correct their strategic course.\n"},{"uri":"/posts/notes-on-a-stack/","title":"Notes on a stack","tags":[],"description":"a simple serverless stack","content":"Authorised Greeting A simple \u0026ldquo;workload\u0026rdquo; for greeting \u0026ldquo;known\u0026rdquo; or \u0026ldquo;unknown\u0026rdquo; visitors to a site.\nIn order to enhance the personality of the public web site: As the business I want to greet unauthorised visitors formally So that politeness is properly observed As the business I want to greet authorised visitors informally So camaraderie is enriched Scenario:users of the API are greeted Background: Given the API is available Given a visitor is \u0026lt;auth_level\u0026gt;* they request a greeting When they are greeted Then the greeting is \u0026lt;greeting\u0026gt;Examples: | auth_level| greeting||unauthorised|formal||authorised|informal| Notes for contemplating a simple serverless workload This is a brain dump outlining the minimum required documentation of the behaviour of the example workload. In this case, the example workload is a simple \u0026ldquo;hello world\u0026rdquo; API, which returns a simple JSON object when queried.\nIf the request is \u0026lsquo;authorised\u0026rsquo; the JSON contains some added, \u0026lsquo;sensitive\u0026rsquo; information.\nThe example is developed, provisioned, and maintained using AWS tools such as\n CodeCommit CodeBuild Cloudformation Lambda DynamoDB S3 IAM KMS  A workload\u0026rsquo;s maintenance cycle sequenceDiagram\rParticipant A as Automation\rParticipant B as Integration\rParticipant C as Production\rA -\u0026gt;\u0026gt; B: initiation - provision stacks\rB --\u0026gt; C: implementation - deploy 'workload' to stacks\rC --x A: iteration - improve infrastructure or application\rThe example workload As a visitor to the Public Web Site I want to perform \u0026#39;authenticated actions\u0026#39; So I can interact with my account Given an authenticated user When they request the API \u0026#39;hello\u0026#39; Then the Public Web Site will execute that action graph LR\rpws[Public Web Site]\rguest-only-actions[Guest Only Actions]\rVisitor --\u0026gt; pws\rpws --\u0026gt; Authentication\rAuthentication --\u0026gt; Authenticated\rAuthentication --\u0026gt; Unauthenticated\rAuthenticated --\u0026gt; RBAC\rRBAC --\u0026gt; Action\rUnauthenticated --\u0026gt; RBAC\rRBAC --- guest-only-actions\rRBAC --\u0026gt; Audit\rAudit --\u0026gt; Logging\rAction --\u0026gt; Logging\rsequenceDiagram\rDEV --\u0026gt; SEC: Collaboration to produce secure, reliable implementation\rSEC --\u0026gt; OPS: Collaboration to produce secure, reliable infrastructure\rDEV --\u0026gt; OPS: Feedback from operational readiness testing\rOPS --\u0026gt; DEV: Feedback from incidents, maintenance, logs\rTo ensure an effective and optimised workflow for maintaining the workload, some things should be considered:\n Evolve security in step with application code Evolve infrastructure in step with application code Including known and emergent security standards Providing automated suites to ensure appropriate hardening Including known and emergent performance standards Respond to incidents with expanded testing  Separation of duties for management of the workload graph LR\rclassDef lifecyclePhase fill:#737373\rlog-events(Record events that occur throughout \u0026lt;br/\u0026gt;the life of the workload)\rsubgraph OPS\rmonitor-cost(Monitor the cost of the workload):::lifecyclePhase\rmonitor-health(Monitor the operational health of the workload)\rrespond-improve(Improve the implementation of the workload\u0026lt;br/\u0026gt; in response to incidents and analytics)\rend\rsubgraph SEC\rlog-audit(Record interactions \u0026lt;br/\u0026gt;with the workload's API)\rprovide-security-tests(Provide an evolving set\u0026lt;br/\u0026gt; of tests to ensure the workload is\u0026lt;br/\u0026gt; secured according to policy)\rend\rsubgraph DEV\rsubgraph IMPLEMENT\rdefine(Define / Develop the workload)\rend\rsubgraph MEASURE\rmeasure-performance(Measure the performance \u0026lt;br/\u0026gt; of the workload)\rend\rsubgraph IMPROVE\rcontinuous-improvement(Evolve the workload\u0026lt;br/\u0026gt; and its infrastructure in step)\rend\rend\rlog-audit --\u0026gt; continuous-improvement\rrespond-improve --\u0026gt; continuous-improvement\rmonitor-cost --\u0026gt; continuous-improvement\rmonitor-health --\u0026gt; continuous-improvement\r"},{"uri":"/posts/begin-with-disaster/","title":"Begin With Disaster","tags":[],"description":"a rant about the way money is wasted","content":"Automate the ability to restore the system from completely nothing In creating a software delivery flow it can be easy to overlook the test-first approach in favour of \u0026ldquo;getting something up and running\u0026rdquo;.\nThis is characterised as \u0026ldquo;business-pressure\u0026rdquo; or \u0026ldquo;time-to-market\u0026rdquo; but neglects a key NFR of most commercially-critical software.\nDisaster Recovery When a new feature (or set of features, eg. a CQRS microservices on k8s) is kicked off, resisting the pressure to deliver a flimsy facade showing some fake data can unlock the more potent business value of disaster recovery.\nSay I have a set of APIs that are to be deployed into the container runtime and their UI effect is to show a list of blog posts. This list is to be obtained from a data store, and served to a client such as a web client or another API.\nTraditionally (or maybe just a lot) the work would commence with the creation of the API projects and they would be developed against Postman or maybe a selenium/BDD style acceptance project.\nIn the crazy new world of devops the things, implementing disaster recovery first makes a lot of sense.\nGiven The first \u0026#34;disaster\u0026#34;: When The feature is entirely missing Then the Feature must be delivered This should cause the designers and engineers to re-frame the initial scope towards infrastructure-as-code, pipelines-as-code etc \u0026ndash; in the event of a disaster, these scripts would be the thing that is to be run again, to set up the services again.\n"},{"uri":"/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/categories/quality-devops/","title":"quality, devops","tags":[],"description":"","content":""},{"uri":"/posts/test-is-a-four-letter-word/","title":"TEST is a four letter word.","tags":[],"description":"","content":"These days it is so easy to tie ourselves up in knots with one simple word. Testing is something that ought to be a part of the fabric of the delivery cycle, the skein of the SDLC.\nAnalysts should be testing their requirements against both the business and the delivery teams, frequently asking \u0026ldquo;is this what you want?\u0026rdquo; and \u0026ldquo;can you build this?\u0026rdquo; so as to gain rapid feedback about the direction they are leading the software.\nDevelopers should be writing tests against their units of work, in order to prove that each unit complies with the requirements (specifications) gathered by the analysts, and that it does not fail with implementation errors.\nQuality Assurance analysts should be testing the features as presented by the devs in order to provide confidence to the business that they are getting what they have asked for.\nSoftware engineers in test (SET or Automation Testers) should be testing the software for behavioural and data integrity, to protect against regressions and defects as the software nears the release point.\nAcceptance testers (business users) should test the software against a set of expectations that a user should be able to operate the software as designed, documented and advertised.\nThe business itself should test the software against the assumption that it actually solves a real-world problem, fills a niche, or just generally is worth selling.\nAll this is Testing. A miniature Bullmer stomps about on my blog-stage ranting, \u0026ldquo;Testing testing testing testing testing\u0026hellip;..\u0026rdquo; - because the word has become so diluted as to be pretty much meaningless now.\nYES! We need to TEST!! Now we have agreed that we need to test, the business asks that we undergo a transformation. Developers, please use TDD to develop your work. But developer \u0026ldquo;X\u0026rdquo; says, \u0026ldquo;I\u0026rsquo;d prefer to use BDD for my work\u0026rdquo;. Because we\u0026rsquo;re all petrified of making \u0026ldquo;X\u0026rdquo; uncomfortable (we don\u0026rsquo;t push people out of their comfort zone any more apparently) - we say \u0026ldquo;ok \u0026hellip; um how is that different from TDD?\u0026rdquo;\nDeveloper \u0026ldquo;X\u0026rdquo; says, \u0026ldquo;I write Given When Then statements and run a cucumber script\u0026rdquo;.\nOh! That\u0026rsquo;s what the automation testers are doing!. Great! Does that mean you are helping them to write the acceptance tests?\n\u0026ldquo;No. It\u0026rsquo;s different.\u0026rdquo;\nDeveloper \u0026ldquo;Y\u0026rdquo; says, \u0026ldquo;I use spock\u0026rdquo;. We groan. Why can\u0026rsquo;t they see? Why don\u0026rsquo;t they understand the stratification? Software is layers of implementation - and each layer is \u0026hellip; \u0026ldquo;tested\u0026rdquo; using a slightly different protocol.\nA rant\u0026hellip; TDD, BDD, ATDD, CDD ... All these things exist because thought-leaders have found\rthat they needed a slightly different pattern to provide confidence in the area\rthey were working on at the time.\rTDD is for units. Bottom up. xUnit.\rBDD is for features. Top down. Cukes, Spock.\rATDD is for Teams - cycles of BDD/TDD to provide completion insight. Scrum/XP.\rCDD is for integration proofing. Pact, Wiremock.\r They aren\u0026rsquo;t like hair-styles or OS - they are purpose-specific. The type of testing you choose should be dependent on what you are doing.\nOk where were we\u0026hellip; We inform the automation testers, \u0026ldquo;you are software engineers you know!\u0026rdquo; (they say \u0026ldquo;wow! we\u0026rsquo;ve always been sneered at by the \u0026lsquo;real\u0026rsquo; engineers!\u0026rdquo; - \u0026ldquo;so you need to write your acceptance tests using a TDD approach\u0026rdquo; \u0026hellip;\n\u0026ldquo;Oh - but why would you need to test the tests?\u0026rdquo; At this point, the transformation coach must pause. It is tempting here to launch into a long rant about how TDD is about providing self-checks, and how do you know your code is working right? etc.\nThis is the moment where it becomes clear that they have not yet had the \u0026ldquo;ah-ha\u0026rdquo; lightbulb that informs them why TDD is not \u0026ldquo;testing the tests\u0026rdquo;.\nAnd it is here where we realise that TEST is a four-letter-word.\nWe need new words. \u0026ldquo;Testing\u0026rdquo; is a shorthand way of describing the processes, controls and self-checks each member of the team undergoes during the delivery cycle, something each member does in a slightly different way - but which ultimately is, as they say, \u0026ldquo;Testing\u0026rdquo;.\n"},{"uri":"/tags/continuous-delivery/","title":"Continuous Delivery","tags":[],"description":"","content":""},{"uri":"/tags/continuous-integration/","title":"Continuous Integration","tags":[],"description":"","content":""},{"uri":"/categories/devops/","title":"devops","tags":[],"description":"","content":""},{"uri":"/tags/learning/","title":"Learning","tags":[],"description":"","content":""},{"uri":"/posts/real-blog/","title":"Reliable, Evolving, Agile, Lean","tags":["Learning","Continuous Integration","Continuous Delivery"],"description":"","content":"./realblog Reliable Evolving Agile Lean  Warning: Star Wars Quotes\n Software that survives. A lot of software today is not given a chance to survive. It\u0026rsquo;s written up, posted on the web, and taken down all within days. This kind of code is \u0026lsquo;firefly code\u0026rsquo; and sometimes that\u0026rsquo;s what we need to get our message across. We know it\u0026rsquo;s only just barely working. We have factored that into our plan.\nSometimes, however, software is sold either as a service or system. If the business wishes to retain consumers, one way to put downward pressure on churn rates is through-\nReliability  \u0026ldquo;Try not: do or do not, there is no try.\u0026rdquo;\n Software that is Reliable can be sold with Confidence. Confidence is one of those amazing fuels which propel the software into the wider arena. When consumers of software are confident in the software, their productivity is unimpeded by the software. When producers of the software are confident in the software, the producers are able to make claims about the software which will attract more consumers to the software.\nAn organisation that offers reliability packaged into its array of products and services will attract repeat business, and reputation will build for that organisation whose software is rigorously tested before it is released onto the market.\nEvolution  \u0026ldquo;Be mindful of your surroundings\u0026rdquo;\n Software that is evolving is malleable, responding to external pressure. It is the harnessing of this property that allows producers of software to react to business forces. A SOLID foundation will prepare the software for continued malleability, allowing it to grow and change with the consumers of that software.\nEvolution underpins the software\u0026rsquo;s ability to remain relevant. To stay in use, software needs to add and remove features and sometimes split or merge with other software. Business systems utilise software ecosystems and the application of certain survival traits found in observed ecosystems will aid the software in its continued survival.\nAgility  \u0026ldquo;Feel, don\u0026rsquo;t think\u0026rdquo;\n Software Agility is found when application code follows a set of design principles which provide patterns for first-commit implementations. Patterns like Singleton, Active Record, Composite Object and Factory can greatly improve a team\u0026rsquo;s ability not only to develop applications quickly, but to effectively communicate through a shared lexicon.\nUsing a suite of unit tests to document application design bridges the gap between consumer and provider design documentation. Using language that is shared across disciplines (such as As, I, So, and Given, When, Then) for design feature descriptions, linking requirements to code using a bridging language like Gherkin to describe in detail the consumer scenarios the application is required to perform facilitates the clarity of communication required to effectively produce software that will not only survive, but thrive.\nUsing a common, portable design language from requirements through development to delivery is key to controlling the inevitable variance (from idea to reality) encountered during the production of a software system. Those simple phrasings listed above can bind the vision to the reality in a way that is not possible otherwise. Providing a set of automated proofs that each design statement (or acceptance criterion) has been adequately met by the software gives all stakeholders confidence that the software is meeting the needs of the consumer.\nLeanitude  \u0026ldquo;Quicker, easier, more seductive the dark side is.\u0026rdquo;\n If we read our scalable_architecture feature we will find that architectures that are implemented \u0026ldquo;because we might strike it lucky\u0026rdquo; actually just drain money from our bank account, and don\u0026rsquo;t provide us with any value.\n\nThe feature describes the behaviour of IT/OPS departments who over-provision their infrastructure based on the idea that a one-time-spike in load might \u0026ldquo;bring down the site\u0026rdquo;. As we can see from the codeship status - this fails to deliver on leanitude.\nMaintaining a Lean Attitude throughout design and construction phase is key to providing an agile, evolving and reliable system that has a high likelihood of survival. This kind of attitude informs the designers and implementers as to the patterns they choose when developing the services representing the feature-set of both the inward and outward interface to the application.\nLeanitude gives construction and operations the freedom to question everything during maintenance of an application: from how a message bus is implemented, to \u0026ldquo;do we even need a message bus?\u0026rdquo;. In development, importance is placed on how to remove code (using a combination of YAGNI and DRY). In operations, this means using architectures highly tuned to the current load. Leanitude says, \u0026ldquo;We don\u0026rsquo;t overprovision, we scale\u0026rdquo;.\nREAL software  \u0026ldquo;When 900 years old you reach\u0026hellip;\u0026rdquo;\n REAL software is software that uses four key techniques from the real world, as observed in successful inhabitants of planet Earth. These four techniques when combined with principles from DevOps, Scrum, ATDD and LEAN can help spin up a quality product and keep it running. Apply a little thought to this formula:\n// some code (I\u0026#39;m told code in posts is cool)  var LEANITUDE = \u0026#34;COOL\u0026#34;; var COOL = \u0026#34;COOL\u0026#34;; describe(\u0026#34;Leanitude\u0026#34;), function () { it(\u0026#34;has a cool name\u0026#34;, function () { expect(LEANITUDE).toBe(COOL); }); }; "},{"uri":"/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"/posts/code-coverage/","title":"100% Coverage is only the beginning","tags":[],"description":"","content":"Code coverage gives us a solid foundation for delivery confidence, but it\u0026rsquo;s not the only way to boost delivery confidence. Confidence: the goal of a quality-focused delivery team. Code confidence is attained through rigorous and frequently repeated testing of the system against a raft of behavioural expectations.\nThese expectations form the specifications of the system and in fact document the system: both the functioning system, and the desires of the business that commissioned the system.\nWhat it says on the tin When our product or service is pitched by the sales team, we want them to be say amazing things about what we do - we also really want them to be telling the truth. They, on the other hand, really want this thing to work - reputation is key to churn reduction.\nTo adequately provide Confidence to our client (the business) that they are getting what they paid for, we run a series of user acceptance tests around the system to prove the functioning system meets the needs of the client.\nBut if it\u0026rsquo;s past its date? Experience has taught us, however, that automated acceptance test suites frequently fall short of covering adequately the full set of operational conditions and as a result we have bugs, or worse - outages.\nPreventing the business from taking money is the worst kind of failure so we should take steps to ensure that we have tried really really hard to prevent this outcome; to ensure the system works.\nSedimentation Unit Testing is a way for the developers to provide confidence to the client (and to themselves) that their code is functioning as desired, and unit testing applied to any given feature is the most effective way to document and explain why we are confident. As a by-product we are documenting the extent to which we have tested the feature.\nThis rigour-metric is the single most important metric for a delivery team as it represents the first indicator that when something goes wrong, and it will - that \u0026lsquo;we took all necessary steps to mitigate the risk of failure\u0026rsquo;.\nIIF we have 100% coverage, we have applied our minds to the task of \u0026ldquo;checking our work\u0026rdquo; in a way that highlights all the paths each branch of logic can take, how we handle exceptions, how we design for dependencies - achieving 100% coverage is how we ask ourselves, \u0026ldquo;is this what I wanted to do?\u0026rdquo;\nOnce we have 100% coverage, we can turn our thoughts to the real question, have we covered all the data permutations adequately? "}]